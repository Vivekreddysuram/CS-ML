{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transformations\n",
        "\n",
        "transformation = transformations.ToTensor()\n",
        "dataset_path = '/path/to/dataset'\n",
        "mnist_train_set = torchvision.datasets.MNIST(root=dataset_path, train=True, transform=transformation, download=True)\n",
        "mnist_test_set = torchvision.datasets.MNIST(root=dataset_path, train=False, transform=transformation, download=False)\n",
        "\n",
        "mnist_validation_set = Subset(mnist_train_set, torch.arange(10000))\n",
        "mnist_training_set = Subset(mnist_train_set, torch.arange(10000, len(mnist_train_set)))\n",
        "\n",
        "batch_size = 64\n",
        "training_loader = DataLoader(mnist_training_set, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(mnist_validation_set, batch_size=batch_size, shuffle=False)\n",
        "testing_loader = DataLoader(mnist_test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, kernel_size_second_layer):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.first_layer = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.second_layer = nn.Sequential(\n",
        "            nn.Conv2d(4, 2, kernel_size=kernel_size_second_layer, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.additional_pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.compute_output_size(kernel_size_second_layer)\n",
        "\n",
        "    def compute_output_size(self, kernel_size_second_layer):\n",
        "        output_dimension = (14 - kernel_size_second_layer) + 1\n",
        "        output_dimension = output_dimension // 2\n",
        "        output_dimension = output_dimension // 2\n",
        "        num_features = 2 * output_dimension * output_dimension\n",
        "        self.fully_connected = nn.Linear(num_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first_layer(x)\n",
        "        first_layer_out_size = x.size()\n",
        "        x = self.second_layer(x)\n",
        "        second_layer_out_size = x.size()\n",
        "        x = self.additional_pooling(x)\n",
        "        additional_pooling_out_size = x.size()\n",
        "        x = x.view(x.size(0), -1)\n",
        "        final_out_size = x.size()\n",
        "        x = self.fully_connected(x)\n",
        "        return x, first_layer_out_size, second_layer_out_size, additional_pooling_out_size, final_out_size\n",
        "\n",
        "def evaluate_on_test_set(model, loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            outputs, *_ = model(imgs)\n",
        "            _, predictions = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "def train_and_validate(model, train_loader, valid_loader, test_loader, epochs=10, kernel_size=3):\n",
        "    print(model)\n",
        "\n",
        "    dummy_input = torch.randn(64, 1, 28, 28)\n",
        "    _, first_layer_out_size, second_layer_out_size, additional_pooling_out_size, final_out_size = model(dummy_input)\n",
        "    print(f'Output Size after First Layer: {first_layer_out_size}')\n",
        "    print(f'Output Size after Second Layer: {second_layer_out_size}')\n",
        "    print(f'Output Size after Additional Pooling: {additional_pooling_out_size}')\n",
        "    print(f'Output Size before Fully Connected Layer: {final_out_size}')\n",
        "    print(f\"\\nStarting training with kernel size {kernel_size} on second layer.\")\n",
        "\n",
        "    optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        epoch_start = time.time()\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            outputs = model(imgs)[0]\n",
        "            loss = loss_function(outputs, labels)\n",
        "            optimiser.zero_grad()\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted_labels = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted_labels == labels).sum().item()\n",
        "\n",
        "        training_accuracy = 100 * train_correct / train_total\n",
        "        training_duration = time.time() - epoch_start\n",
        "\n",
        "        model.eval()\n",
        "        validation_loss = 0.0\n",
        "        valid_correct = 0\n",
        "        valid_total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in valid_loader:\n",
        "                outputs = model(imgs)[0]\n",
        "                loss = loss_function(outputs, labels)\n",
        "                validation_loss += loss.item()\n",
        "                _, predicted_labels = torch.max(outputs.data, 1)\n",
        "                valid_total += labels.size(0)\n",
        "                valid_correct += (predicted_labels == labels).sum().item()\n",
        "\n",
        "        validation_accuracy = 100 * valid_correct / valid_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader):.4f}, '\n",
        "              f'Training Accuracy: {training_accuracy:.2f}%, '\n",
        "              f'Validation Loss: {validation_loss/len(valid_loader):.4f}, '\n",
        "              f'Validation Accuracy: {validation_accuracy:.2f}%, '\n",
        "              f'Time: {training_duration:.2f}s')\n",
        "\n",
        "    evaluate_on_test_set(model, test_loader)\n",
        "\n",
        "model_instance_3 = ConvNet(kernel_size_second_layer=3)\n",
        "train_and_validate(model_instance_3, training_loader, validation_loader, testing_loader, epochs=10, kernel_size=3)\n",
        "\n",
        "model_instance_5 = ConvNet(kernel_size_second_layer=5)\n",
        "train_and_validate(model_instance_5, training_loader, validation_loader, testing_loader, epochs=10, kernel_size=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSc676vkjgWz",
        "outputId": "c207cc41-512b-42f1-a630-9af8ac31559b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet(\n",
            "  (first_layer): Sequential(\n",
            "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (second_layer): Sequential(\n",
            "    (0): Conv2d(4, 2, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (additional_pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fully_connected): Linear(in_features=18, out_features=10, bias=True)\n",
            ")\n",
            "Output Size after First Layer: torch.Size([64, 4, 14, 14])\n",
            "Output Size after Second Layer: torch.Size([64, 2, 6, 6])\n",
            "Output Size after Additional Pooling: torch.Size([64, 2, 3, 3])\n",
            "Output Size before Fully Connected Layer: torch.Size([64, 18])\n",
            "\n",
            "Starting training with kernel size 3 on second layer.\n",
            "Epoch 1, Training Loss: 1.1884, Training Accuracy: 60.43%, Validation Loss: 0.6303, Validation Accuracy: 80.23%, Time: 10.02s\n",
            "Epoch 2, Training Loss: 0.5390, Training Accuracy: 82.94%, Validation Loss: 0.4971, Validation Accuracy: 84.54%, Time: 9.97s\n",
            "Epoch 3, Training Loss: 0.4448, Training Accuracy: 86.01%, Validation Loss: 0.4225, Validation Accuracy: 86.65%, Time: 9.95s\n",
            "Epoch 4, Training Loss: 0.3938, Training Accuracy: 87.65%, Validation Loss: 0.3824, Validation Accuracy: 88.09%, Time: 9.89s\n",
            "Epoch 5, Training Loss: 0.3645, Training Accuracy: 88.75%, Validation Loss: 0.3547, Validation Accuracy: 89.10%, Time: 9.95s\n",
            "Epoch 6, Training Loss: 0.3463, Training Accuracy: 89.28%, Validation Loss: 0.3392, Validation Accuracy: 89.63%, Time: 10.09s\n",
            "Epoch 7, Training Loss: 0.3332, Training Accuracy: 89.61%, Validation Loss: 0.3273, Validation Accuracy: 90.17%, Time: 9.94s\n",
            "Epoch 8, Training Loss: 0.3233, Training Accuracy: 89.99%, Validation Loss: 0.3189, Validation Accuracy: 90.36%, Time: 9.96s\n",
            "Epoch 9, Training Loss: 0.3153, Training Accuracy: 90.28%, Validation Loss: 0.3132, Validation Accuracy: 90.29%, Time: 10.05s\n",
            "Epoch 10, Training Loss: 0.3070, Training Accuracy: 90.59%, Validation Loss: 0.3034, Validation Accuracy: 90.99%, Time: 9.91s\n",
            "Test Accuracy: 90.90%\n",
            "ConvNet(\n",
            "  (first_layer): Sequential(\n",
            "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (second_layer): Sequential(\n",
            "    (0): Conv2d(4, 2, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (additional_pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fully_connected): Linear(in_features=8, out_features=10, bias=True)\n",
            ")\n",
            "Output Size after First Layer: torch.Size([64, 4, 14, 14])\n",
            "Output Size after Second Layer: torch.Size([64, 2, 5, 5])\n",
            "Output Size after Additional Pooling: torch.Size([64, 2, 2, 2])\n",
            "Output Size before Fully Connected Layer: torch.Size([64, 8])\n",
            "\n",
            "Starting training with kernel size 5 on second layer.\n",
            "Epoch 1, Training Loss: 1.2612, Training Accuracy: 57.82%, Validation Loss: 0.7073, Validation Accuracy: 78.29%, Time: 10.27s\n",
            "Epoch 2, Training Loss: 0.6625, Training Accuracy: 79.76%, Validation Loss: 0.5964, Validation Accuracy: 82.05%, Time: 10.23s\n",
            "Epoch 3, Training Loss: 0.5847, Training Accuracy: 82.02%, Validation Loss: 0.5348, Validation Accuracy: 83.79%, Time: 10.26s\n",
            "Epoch 4, Training Loss: 0.5411, Training Accuracy: 83.40%, Validation Loss: 0.4997, Validation Accuracy: 84.70%, Time: 10.41s\n",
            "Epoch 5, Training Loss: 0.5139, Training Accuracy: 84.17%, Validation Loss: 0.4814, Validation Accuracy: 85.44%, Time: 10.35s\n",
            "Epoch 6, Training Loss: 0.4938, Training Accuracy: 84.82%, Validation Loss: 0.4700, Validation Accuracy: 85.32%, Time: 10.19s\n",
            "Epoch 7, Training Loss: 0.4815, Training Accuracy: 85.13%, Validation Loss: 0.4605, Validation Accuracy: 86.02%, Time: 10.19s\n",
            "Epoch 8, Training Loss: 0.4701, Training Accuracy: 85.50%, Validation Loss: 0.4527, Validation Accuracy: 85.88%, Time: 10.16s\n",
            "Epoch 9, Training Loss: 0.4614, Training Accuracy: 85.80%, Validation Loss: 0.4378, Validation Accuracy: 86.71%, Time: 10.33s\n",
            "Epoch 10, Training Loss: 0.4525, Training Accuracy: 86.00%, Validation Loss: 0.4265, Validation Accuracy: 86.94%, Time: 10.28s\n",
            "Test Accuracy: 86.76%\n"
          ]
        }
      ]
    }
  ]
}